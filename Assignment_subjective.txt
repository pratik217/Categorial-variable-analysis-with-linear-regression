1. From your analysis of the categorical variables from the dataset, what could you infer about their effect on the dependent variable?

Ans. Categorical Variables
     1. Season
        Thers no any much difference between the dependent/target variable (cnt) in the Summer, Fall and Winter season. We can see any huge diffrence only in Spring
        season which is too small as compared to the rest.
     2. 'yr'
        Its clear to see that there has been a narrow increase or rise of the quartile of cnt in 2019 as compared to 2018.
     
     3. 'mnth'
        The effect by this variable is just detaile view of season variable, where we can see a gradual increase in the quartiles.
     4. 'Holiday'
         Thers no any diffrence in the high quartile of both the sub-variables. Just the range of quartile is a bit bigger than the days of no holiday.

     5. 'Weekday'
         The quartile range of the whole week is somewhat same.

     6. 'Workngday'
         No nay diffrence.
     7. 'Weathersit'
        Theres a high fall of quartile range in rainy season.

2.  Why is it important to use drop_first=True during dummy variable creation?

Ans. When a dummy col is formed, we do not need the first column to describe the columns. To make the dataFrame short and easy to underdstand drop_first is used.

3. Looking at the pair-plot among the numerical variables, which one has the highest correlation with the target variable?

Ans. atemp and registered columnbs has the highest correlation with the target variable cnt. So the fisr and foremost task is to drop both the columns
     from the dataFrame.

4. How did you validate the assumptions of Linear Regression after building the model on the training set?

Ans. Linear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y.
Independence: The residuals are independent.
Homoscedasticity: The residuals have constant variance at every level of x.
Normality: The residuals of the model are normally distributed.

5. Based on the final model, which are the top 3 features contributing significantly towards explaining the demand of the shared bikes?

Ans.  windspeed, season_spring and season_winter are the top three features contributing significantly.

1. Explain the linear regression algorithm in detail.

ans. Linear Regression Algorithm is a machine learning algorithm based on supervised learning. Linear regression is a part of regression
     analysis. Regression analysis is a technique of predictive modelling that helps you to find out the relationship between
     Input and the target variable.
2. Explain the Anscombe’s quartet in detail.

Ans.  Anscombe's Quartet can be defined as a group of four data sets which are nearly identical in simple descriptive statistics,
      but there are some peculiarities in the dataset that fools the regression model if built. They have very different distributions
      and appear differently when plotted on scatter plots.

3. What is Pearson’s R?

Ans.  The correlation between two variables reflects the degree to which the variables are related. When computed in a sample, 
      t is designated by the letter "r" and is sometimes called "Pearson's r." Pearson's correlation reflects the degree
      of linear relationship between two variables.

4. What is scaling? Why is scaling performed? What is the difference between normalized scaling and standardized scaling?

Ans.  Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing
      it is also known as data normalization and is generally performed during the data preprocessing step.
      It is performed during the data pre-processing to handle highly varying magnitudes or values or units.
      Normalization usually means to scale a variable to have a values between 0 and 1, while standardization
      ransforms data to have a mean of zero and a standard deviation of 1.

5. You might have observed that sometimes the value of VIF is infinite. Why does this happen?

Ans.   If there is perfect correlation, then VIF = infinity. A large value of VIF indicates that there is a correlation between
       the variables. If the VIF is 4, this means that the variance of the model coefficient s inflated by a factor of 
       4 due to the presence of multicollinearity.

6. What is a Q-Q plot? Explain the use and importance of a Q-Q plot in linear regression.

Ans.  In statistics, a Q–Q plot is a probability plot, which is a graphical method for comparing two probability
      distributions by plotting their quantiles against each other. 
      The purpose of Q Q plots is to find out if two sets of data come from the same distribution. A 45 degree angle
      is plotted on the Q Q plot; if the two data sets come from a common distribution, the points will fall on that
      reference line.
